# ================================================================
# ðŸ§  100 Days of ML for Materials Science
# Day 10 â€” Predicting Ionic Conductivity of Solid Electrolytes
# Author: [Your Name]
# ================================================================

# --- 1. Import libraries ---
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_absolute_error

# --- 2. Create a small toy dataset (simulated solid electrolytes) ---
data = {
    "Material": ["LLZO", "LLTO", "LATP", "LAGP", "PEO+LLZO", "LLZO-Al", "LLZO-Ga", "PEO+LLTO", "PEO+LATP", "LLZO+PIL"],
    "Density": [5.1, 4.8, 4.5, 4.6, 3.2, 5.05, 5.0, 3.1, 3.3, 3.4],
    "Bandgap": [5.4, 3.2, 4.1, 4.3, 5.0, 5.5, 5.6, 3.5, 4.2, 4.8],
    "IonicRadius": [0.76, 0.90, 0.74, 0.72, 0.80, 0.75, 0.76, 0.88, 0.73, 0.79],
    # Conductivity in S/cm (simulated trends)
    "Conductivity": [1.2e-3, 8.5e-4, 3.4e-4, 2.7e-4, 5.6e-4, 1.4e-3, 1.5e-3, 9.1e-4, 4.1e-4, 6.0e-4]
}

df = pd.DataFrame(data)

# --- 3. Explore dataset ---
print("Dataset overview:")
print(df.head(), "\n")
print(df.describe())

# --- 4. Prepare features and target ---
X = df[["Density", "Bandgap", "IonicRadius"]]
y = df["Conductivity"]

# Split into train/test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# --- 5. Train a Random Forest model ---
model = RandomForestRegressor(random_state=42)
model.fit(X_train, y_train)

# --- 6. Predict and evaluate ---
y_pred = model.predict(X_test)

r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)

print("\nModel Performance:")
print(f"RÂ² Score: {r2:.3f}")
print(f"Mean Absolute Error: {mae:.2e}")

# --- 7. Visualize results ---
plt.figure(figsize=(6,5))
plt.scatter(y_test, y_pred, s=80, color='blue', alpha=0.7)
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)
plt.xlabel("True Conductivity (S/cm)")
plt.ylabel("Predicted Conductivity (S/cm)")
plt.title("True vs Predicted Ionic Conductivity")
plt.grid(True)
plt.show()

# --- 8. Feature importance ---
plt.figure(figsize=(6,4))
plt.bar(X.columns, model.feature_importances_, color='teal')
plt.title("Feature Importance for Ionic Conductivity Prediction")
plt.ylabel("Importance")
plt.show()

# --- 9. Interpret ---
importance_df = pd.DataFrame({
    "Feature": X.columns,
    "Importance": model.feature_importances_
}).sort_values(by="Importance", ascending=False)

print("\nFeature importance ranking:")
print(importance_df)

# --- 10. Summary ---
print("""
Summary:
- Built a simple ML model (Random Forest) to predict ionic conductivity of LLZO-based materials.
- Features used: Density, Bandgap, IonicRadius
- The model helps reveal which physical parameters most influence conductivity.
- This simple workflow can scale to larger datasets (e.g., Materials Project or literature data).
""")
# ================================================================
# âœ… Step (a) Cross-Validation + (b) Scaling + Linear Baseline
# ================================================================

from sklearn.model_selection import cross_val_score, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline

# --- Create 5-fold CV splitter ---
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# --- Random Forest CV ---
rf_cv_scores = cross_val_score(model, X, y, cv=kf, scoring='r2')
print("Random Forest CV RÂ²:", np.mean(rf_cv_scores).round(3), "+/-", np.std(rf_cv_scores).round(3))

# --- Linear Regression (with scaling) ---
linear_model = make_pipeline(StandardScaler(), LinearRegression())
lr_cv_scores = cross_val_score(linear_model, X, y, cv=kf, scoring='r2')
print("Linear Regression CV RÂ²:", np.mean(lr_cv_scores).round(3), "+/-", np.std(lr_cv_scores).round(3))

# ================================================================
# âœ… Step (c) Increase dataset size (synthetic expansion)
# ================================================================

def augment_dataset(df, n=5, noise_level=0.05):
    augmented = []
    for _, row in df.iterrows():
        for _ in range(n):
            new_row = row.copy()
            new_row["Density"] += np.random.uniform(-noise_level, noise_level)
            new_row["Bandgap"] += np.random.uniform(-noise_level, noise_level)
            new_row["IonicRadius"] += np.random.uniform(-0.02, 0.02)
            new_row["Conductivity"] += np.random.uniform(-noise_level*row["Conductivity"], noise_level*row["Conductivity"])
            augmented.append(new_row)
    return pd.DataFrame(augmented)

aug_df = augment_dataset(df, n=5)
print(f"Original data size: {len(df)}, Augmented size: {len(aug_df)}")

# Combine both
full_df = pd.concat([df, aug_df], ignore_index=True)

# Retrain with larger dataset
X_full = full_df[["Density", "Bandgap", "IonicRadius"]]
y_full = full_df["Conductivity"]

model.fit(X_full, y_full)
rf_cv_scores_large = cross_val_score(model, X_full, y_full, cv=kf, scoring='r2')
print("Random Forest (augmented data) CV RÂ²:", np.mean(rf_cv_scores_large).round(3))
